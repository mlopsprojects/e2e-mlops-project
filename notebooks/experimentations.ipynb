{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from mlflow.models import infer_signature\n",
    "from pprint import pprint\n",
    "from typing import Union, Dict, List, Tuple\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import boto3\n",
    "import joblib\n",
    "import os\n",
    "import math\n",
    "import mlflow\n",
    "import optuna\n",
    "import warnings\n",
    "import shutil\n",
    "import ast\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Get the current datetime string for experiment names\n",
    "current_datetime = datetime.now()\n",
    "datetime_string = current_datetime.strftime(\"%Y-%m-%d_%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml_file(path, file):\n",
    "    # reading credentials files\n",
    "    with open(f\"{os.path.join(path, file)}\") as f:\n",
    "        try:\n",
    "            content = yaml.safe_load(f)\n",
    "        except yaml.YAMLError as e:\n",
    "            raise e\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "CONFIG_PATH = os.path.join(\"src\", \"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'src/config/credentials.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m credentials_config \u001b[38;5;241m=\u001b[39m \u001b[43mread_yaml_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcredentials.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m general_settings \u001b[38;5;241m=\u001b[39m read_yaml_file(path\u001b[38;5;241m=\u001b[39mCONFIG_PATH, file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msettings.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m credentials_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEC2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOUR_EC2_INSTANCE_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m, in \u001b[0;36mread_yaml_file\u001b[0;34m(path, file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_yaml_file\u001b[39m(path, file):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# reading credentials files\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m             content \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n",
      "File \u001b[0;32m/opt/venv/_dev/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'src/config/credentials.yaml'"
     ]
    }
   ],
   "source": [
    "# credentials_config = read_yaml_file(path=CONFIG_PATH, file=\"credentials.yaml\")\n",
    "\n",
    "general_settings = read_yaml_file(path=CONFIG_PATH, file=\"settings.yaml\")\n",
    "\n",
    "# if credentials_config[\"EC2\"] != \"YOUR_EC2_INSTANCE_URL\":\n",
    "#     mlflow.set_tracking_uri(f\"http://{credentials_config['EC2']}:5000\")\n",
    "# else:\n",
    "#     mlflow.set_tracking_uri(f\"http://mlflow:5000\")\n",
    "\n",
    "mlflow.set_tracking_uri(f\"http://mlflow:5000\")\n",
    "print(f\"Tracking Server URI: '{mlflow.get_tracking_uri()}'\")\n",
    "\n",
    "SEED = 42\n",
    "ARTIFACTS_OUTPUT_PATH = general_settings[\"ARTIFACTS_PATH\"]\n",
    "FEATURES_OUTPUT_PATH = general_settings[\"FEATURES_PATH\"]\n",
    "RAW_FILE_PATH = os.path.join(\n",
    "    general_settings[\"DATA_PATH\"], general_settings[\"RAW_FILE_NAME\"]\n",
    ")\n",
    "PROCESSED_RAW_FILE = \"Preprocessed_\" + general_settings[\"RAW_FILE_NAME\"]\n",
    "PROCESSED_RAW_FILE_PATH = os.path.join(\n",
    "    general_settings[\"DATA_PATH\"], PROCESSED_RAW_FILE\n",
    ")\n",
    "FEATURE_SELECTION_EXPERIMENT_NAME = f\"feature-selection-experimentation\"\n",
    "HYPERPARAMETER_TUNING_EXPERIMENT_NAME = f\"hyperparameters-tuning-experimentation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the preprocessed dataset from the aws s3 bucket\n",
    "if credentials_config[\"S3\"] != \"YOUR_S3_BUCKET_URL\":\n",
    "    # configuring AWS credentials\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = credentials_config[\"AWS_ACCESS_KEY\"]\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = credentials_config[\"AWS_SECRET_KEY\"]\n",
    "\n",
    "    # downloading preprocessed dataset\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=credentials_config[\"AWS_ACCESS_KEY\"],\n",
    "        aws_secret_access_key=credentials_config[\"AWS_SECRET_KEY\"]\n",
    "    )\n",
    "    s3.download_file(\n",
    "        credentials_config[\"S3\"],\n",
    "        PROCESSED_RAW_FILE,\n",
    "        PROCESSED_RAW_FILE_PATH\n",
    "    )\n",
    "\n",
    "    # downloading artifacts from the aws s3 bucket\n",
    "    !aws s3 cp --recursive s3://{credentials_config[\"S3\"]}/artifacts {ARTIFACTS_OUTPUT_PATH}\n",
    "\n",
    "    # downloading models from the aws s3 bucket\n",
    "    !aws s3 cp --recursive s3://{credentials_config[\"S3\"]}/features {FEATURES_OUTPUT_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading features\n",
    "X_train = joblib.load(os.path.join(FEATURES_OUTPUT_PATH, \"X_train.pkl\"))\n",
    "y_train = joblib.load(os.path.join(FEATURES_OUTPUT_PATH, \"y_train.pkl\"))\n",
    "\n",
    "X_valid = joblib.load(os.path.join(FEATURES_OUTPUT_PATH, \"X_valid.pkl\"))\n",
    "y_valid = joblib.load(os.path.join(FEATURES_OUTPUT_PATH, \"y_valid.pkl\"))\n",
    "\n",
    "# loading artifacts\n",
    "sc = joblib.load(os.path.join(ARTIFACTS_OUTPUT_PATH, \"features_sc.pkl\"))\n",
    "ohe = joblib.load(os.path.join(ARTIFACTS_OUTPUT_PATH, \"features_ohe.pkl\"))\n",
    "ohe_label = joblib.load(os.path.join(ARTIFACTS_OUTPUT_PATH, \"label_ohe.pkl\"))\n",
    "\n",
    "# loading feature columns\n",
    "temp_df = pd.read_csv(PROCESSED_RAW_FILE_PATH, sep=\",\")\n",
    "FEATURES_NAME = temp_df.columns.tolist()\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVE THIS CELL IN ACTUAL EXECUTIONS\n",
    "this is to make execution faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:1000, :10]\n",
    "y_train = y_train[:1000]\n",
    "X_valid = X_valid[:200, :10]\n",
    "y_valid = y_valid[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the baseline models\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "rf = RandomForestClassifier(random_state=SEED, verbose=0, n_jobs=-1)\n",
    "xg = XGBClassifier(random_state=SEED, n_jobs=-1)\n",
    "lg = LGBMClassifier(random_state=SEED, verbose=-1, objective=\"multiclass\")\n",
    "cb = CatBoostClassifier(random_seed=SEED, verbose=0, allow_writing_files=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_selection(\n",
    "    model: Union[\n",
    "        DecisionTreeClassifier,\n",
    "        RandomForestClassifier,\n",
    "        XGBClassifier,\n",
    "        LGBMClassifier,\n",
    "        CatBoostClassifier,\n",
    "    ],\n",
    "    number_features: int,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.array,\n",
    "    X_valid: np.ndarray,\n",
    "    y_valid: np.array,\n",
    ") -> Dict:\n",
    "    # initializing and fitting the sfs class\n",
    "    sfs = SequentialFeatureSelector(model, n_features_to_select=number_features, cv=3)\n",
    "    sfs.fit(X=X_train, y=y_train)\n",
    "\n",
    "    # getting the indexes of the best features\n",
    "    selected_features_indexes = np.argwhere(sfs.get_support()).reshape(-1)\n",
    "\n",
    "    reduced_X_train = sfs.transform(X_train)\n",
    "    reduced_X_valid = sfs.transform(X_valid)\n",
    "\n",
    "    # training the model\n",
    "    model.fit(reduced_X_train, y_train)\n",
    "\n",
    "    # calculating the training f1 score\n",
    "    predicted_y_train = model.predict(reduced_X_train)\n",
    "    train_f1 = f1_score(y_true=y_train, y_pred=predicted_y_train, average=\"weighted\")\n",
    "\n",
    "    # calculating the validation f1 score\n",
    "    predicted_y_valid = model.predict(reduced_X_valid)\n",
    "    valid_f1 = f1_score(y_true=y_valid, y_pred=predicted_y_valid, average=\"weighted\")\n",
    "\n",
    "    # inferring the signature of the trained model\n",
    "    signature = infer_signature(\n",
    "        model_input=reduced_X_train, model_output=predicted_y_train\n",
    "    )\n",
    "\n",
    "    # saving the metrics and artifacts that we want to log in mlflow\n",
    "    selected_features_names = list(\n",
    "        map(lambda i: FEATURES_NAME[i], selected_features_indexes.tolist())\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"train_f1\": train_f1,\n",
    "        \"valid_f1\": valid_f1,\n",
    "        \"features\": selected_features_names,\n",
    "        \"model\": model,\n",
    "        \"model_signature\": signature,\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def set_configurations_mlflow(\n",
    "    model: Union[\n",
    "        DecisionTreeClassifier,\n",
    "        RandomForestClassifier,\n",
    "        XGBClassifier,\n",
    "        LGBMClassifier,\n",
    "        CatBoostClassifier,\n",
    "    ],\n",
    "    y_train: np.array,\n",
    "    y_valid: np.array,\n",
    ") -> Tuple[np.array, np.array, str, str]:\n",
    "    # reshaping the target values (if needed) and setting the run name and which\n",
    "    # flavor is being used for each machine learning model\n",
    "    if isinstance(model, DecisionTreeClassifier):\n",
    "        y_train = np.argmax(y_train, axis=1)\n",
    "        y_valid = np.argmax(y_valid, axis=1)\n",
    "        run_name = \"decision_tree\"\n",
    "        flavor = \"sklearn\"\n",
    "\n",
    "    if isinstance(model, RandomForestClassifier):\n",
    "        run_name = \"random_forest\"\n",
    "        flavor = \"sklearn\"\n",
    "\n",
    "    if isinstance(model, XGBClassifier):\n",
    "        run_name = \"xgboost\"\n",
    "        flavor = \"xgboost\"\n",
    "\n",
    "    if isinstance(model, LGBMClassifier):\n",
    "        y_train = np.argmax(y_train, axis=1)\n",
    "        y_valid = np.argmax(y_valid, axis=1)\n",
    "        run_name = \"lightgbm\"\n",
    "        flavor = \"lightgbm\"\n",
    "\n",
    "    if isinstance(model, CatBoostClassifier):\n",
    "        y_train = np.argmax(y_train, axis=1)\n",
    "        y_valid = np.argmax(y_valid, axis=1)\n",
    "        run_name = \"catboost\"\n",
    "        flavor = \"catboost\"\n",
    "\n",
    "    # disabling some options of the current flavor's autolog\n",
    "    if flavor == \"sklearn\":\n",
    "        mlflow.sklearn.autolog(\n",
    "            log_models=False,\n",
    "            log_post_training_metrics=False,\n",
    "            log_model_signatures=False,\n",
    "            log_input_examples=True,\n",
    "            log_datasets=False,\n",
    "            silent=True,\n",
    "            disable=True,\n",
    "        )\n",
    "    elif flavor == \"xgboost\":\n",
    "        mlflow.xgboost.autolog(\n",
    "            log_models=False,\n",
    "            log_model_signatures=False,\n",
    "            log_input_examples=True,\n",
    "            log_datasets=False,\n",
    "            silent=True,\n",
    "            disable=True,\n",
    "        )\n",
    "    elif flavor == \"lightgbm\":\n",
    "        mlflow.lightgbm.autolog(\n",
    "            log_models=False,\n",
    "            log_model_signatures=False,\n",
    "            log_input_examples=True,\n",
    "            log_datasets=False,\n",
    "            silent=True,\n",
    "            disable=True,\n",
    "        )\n",
    "    elif flavor == \"catboost\":\n",
    "        # there is no autolog implemented for catboost\n",
    "        pass\n",
    "\n",
    "    return y_train, y_valid, run_name, flavor\n",
    "\n",
    "\n",
    "def run_feature_selection_experiment(\n",
    "    models: List,\n",
    "    min_features: int,\n",
    "    max_features: int,\n",
    "    experiment_id: str,\n",
    "    metric_to_optimize: str = \"valid_f1\",\n",
    "    direction: str = \"max\",\n",
    ") -> None:\n",
    "    for model in models:\n",
    "        # reshaping the target values (if needed) and setting some mlflow's configuration\n",
    "        new_y_train, new_y_valid, run_name, flavor = set_configurations_mlflow(\n",
    "            model=model, y_train=y_train, y_valid=y_valid\n",
    "        )\n",
    "        model_type_results = []\n",
    "\n",
    "        # starting a new run for the current model\n",
    "        with mlflow.start_run(experiment_id=experiment_id, run_name=run_name):\n",
    "            pprint(f\"Starting the run for the {run_name} model!\\n\")\n",
    "\n",
    "            for i, n_features in enumerate(range(min_features, max_features + 1)):\n",
    "                # creating a nested run inside the model's main run\n",
    "                with mlflow.start_run(\n",
    "                    experiment_id=experiment_id,\n",
    "                    run_name=f\"{run_name}_experiment_{i}\",\n",
    "                    nested=True,\n",
    "                ) as run:\n",
    "                    # running the feature selection main function\n",
    "                    results = apply_feature_selection(\n",
    "                        model=model,\n",
    "                        number_features=n_features,\n",
    "                        X_train=X_train,\n",
    "                        y_train=new_y_train,\n",
    "                        X_valid=X_valid,\n",
    "                        y_valid=new_y_valid,\n",
    "                    )\n",
    "\n",
    "                    # logging the trained model\n",
    "                    if flavor == \"sklearn\":\n",
    "                        mlflow.sklearn.log_model(\n",
    "                            results[\"model\"],\n",
    "                            run_name,\n",
    "                            signature=results[\"model_signature\"],\n",
    "                        )\n",
    "                        # logging the model\"s default parameters\n",
    "                        mlflow.log_params(results[\"model\"].get_params(deep=True))\n",
    "                    elif flavor == \"xgboost\":\n",
    "                        mlflow.xgboost.log_model(\n",
    "                            results[\"model\"],\n",
    "                            run_name,\n",
    "                            signature=results[\"model_signature\"],\n",
    "                        )\n",
    "                        # logging the model's default parameters\n",
    "                        mlflow.log_params(results[\"model\"].get_params(deep=True))\n",
    "                    elif flavor == \"lightgbm\":\n",
    "                        mlflow.lightgbm.log_model(\n",
    "                            results[\"model\"],\n",
    "                            run_name,\n",
    "                            signature=results[\"model_signature\"],\n",
    "                        )\n",
    "                        # logging the model's default parameters\n",
    "                        mlflow.log_params(results[\"model\"].get_params())\n",
    "                    elif flavor == \"catboost\":\n",
    "                        mlflow.catboost.log_model(\n",
    "                            results[\"model\"],\n",
    "                            run_name,\n",
    "                            signature=results[\"model_signature\"],\n",
    "                        )\n",
    "                        # logging the model's default parameters\n",
    "                        mlflow.log_params(results[\"model\"].get_all_params())\n",
    "\n",
    "                    # logging the training and validation scores\n",
    "                    mlflow.log_metric(\"train_f1\", results[\"train_f1\"])\n",
    "                    mlflow.log_metric(\"valid_f1\", results[\"valid_f1\"])\n",
    "\n",
    "                    # logging the artifacts (original dataset, features, and encoders objects)\n",
    "                    mlflow.log_artifact(PROCESSED_RAW_FILE_PATH)\n",
    "                    mlflow.log_artifact(ARTIFACTS_OUTPUT_PATH)\n",
    "                    mlflow.log_artifact(FEATURES_OUTPUT_PATH)\n",
    "\n",
    "                    # logging the indexes of the best features\n",
    "                    mlflow.log_param(\"features\", results[\"features\"])\n",
    "\n",
    "                    # add mlflow ids to results object and append to list\n",
    "                    results[\"experiment_id\"] = experiment_id\n",
    "                    results[\"run_name\"] = f\"{run_name}_experiment_{i}\"\n",
    "                    results[\"flavor\"] = flavor\n",
    "                    results[\"run_id\"] = run.info.run_id\n",
    "                    model_type_results.append(results)\n",
    "\n",
    "        # register the best feature selection in mlflow\n",
    "        df = pd.DataFrame.from_records(model_type_results)\n",
    "        if direction == \"min\":\n",
    "            best_result = df.loc[df[metric_to_optimize].idxmin()]\n",
    "        elif direction == \"max\":\n",
    "            best_result = df.loc[df[metric_to_optimize].idxmax()]\n",
    "        else:\n",
    "            raise NotImplementedError(\"\")\n",
    "        tags = {\"type\": \"baseline\", \"model\": run_name}\n",
    "        result = mlflow.register_model(\n",
    "            model_uri=f\"runs:/{best_result['run_id']}/{best_result['run_name']}\",\n",
    "            name=f\"{FEATURE_SELECTION_EXPERIMENT_NAME}_{run_name}\",\n",
    "            tags=tags,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [dt, rf, xg, lg, cb]\n",
    "min_features = math.floor(X_train.shape[1] * 0.2)\n",
    "max_features = math.floor(X_train.shape[1] * 0.5)\n",
    "\n",
    "# creating a new mlflow's experiment\n",
    "experiment_id = mlflow.create_experiment(\n",
    "    name=FEATURE_SELECTION_EXPERIMENT_NAME + \"_\" + datetime_string,\n",
    ")\n",
    "\n",
    "# running the feature selection experiments\n",
    "run_feature_selection_experiment(\n",
    "    models=models,\n",
    "    min_features=min_features,\n",
    "    max_features=max_features,\n",
    "    experiment_id=experiment_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective:\n",
    "    def __init__(\n",
    "        self,\n",
    "        run_group_name: str,\n",
    "        experiment_id: str,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.array,\n",
    "        X_valid: np.ndarray,\n",
    "        y_valid: np.array,\n",
    "        indexes: List,\n",
    "    ) -> None:\n",
    "        self.run_group_name = run_group_name\n",
    "        self.experiment_id = experiment_id\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_valid = X_valid\n",
    "        self.y_valid = y_valid\n",
    "        self.indexes_name = indexes\n",
    "        self.indexes = [FEATURES_NAME.index(i) for i in indexes]\n",
    "\n",
    "        if self.run_group_name in [\"decision_tree\", \"lightgbm\", \"catboost\"]:\n",
    "            self.y_train = np.argmax(self.y_train, axis=1)\n",
    "            self.y_valid = np.argmax(self.y_valid, axis=1)\n",
    "\n",
    "        self.X_train = self.X_train[:, self.indexes]\n",
    "        self.X_valid = self.X_valid[:, self.indexes]\n",
    "\n",
    "    def __call__(self, trial: optuna.trial.Trial) -> float:\n",
    "        with mlflow.start_run(\n",
    "            experiment_id=self.experiment_id,\n",
    "            run_name=f\"{self.run_group_name}_trial_{trial.number}\",\n",
    "            nested=True,\n",
    "        ) as run:\n",
    "            trial.set_user_attr(\"run_id\", run.info.run_id)\n",
    "            trial.set_user_attr(\"run_name\", run.info.run_name)\n",
    "            if self.run_group_name == \"decision_tree\":\n",
    "                params = {\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 2, 32, step=2),\n",
    "                    \"min_samples_split\": trial.suggest_int(\n",
    "                        \"min_samples_split\", 2, 8, step=1\n",
    "                    ),\n",
    "                    \"min_samples_leaf\": trial.suggest_int(\n",
    "                        \"min_samples_leaf\", 1, 6, step=1\n",
    "                    ),\n",
    "                    \"min_weight_fraction_leaf\": trial.suggest_float(\n",
    "                        \"min_weight_fraction_leaf\", 0, 0.5, step=0.1\n",
    "                    ),\n",
    "                    \"max_leaf_nodes\": trial.suggest_int(\n",
    "                        \"max_leaf_nodes\", 2, 16, step=2\n",
    "                    ),\n",
    "                    \"random_state\": SEED,\n",
    "                }\n",
    "                model = DecisionTreeClassifier(**params)\n",
    "\n",
    "            if self.run_group_name == \"random_forest\":\n",
    "                params = {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 10, 50),\n",
    "                    \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 32),\n",
    "                    \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 32),\n",
    "                    \"random_state\": SEED,\n",
    "                    \"n_jobs\": -1,\n",
    "                }\n",
    "                model = RandomForestClassifier(**params)\n",
    "\n",
    "            if self.run_group_name == \"xgboost\":\n",
    "                params = {\n",
    "                    \"booster\": trial.suggest_categorical(\n",
    "                        \"booster\", [\"gbtree\", \"gblinear\", \"dart\"]\n",
    "                    ),\n",
    "                    \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "                    \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "                    \"random_state\": SEED,\n",
    "                    \"n_jobs\": -1,\n",
    "                }\n",
    "                model = XGBClassifier(**params)\n",
    "\n",
    "            if self.run_group_name == \"lightgbm\":\n",
    "                params = {\n",
    "                    \"objective\": \"multiclass\",\n",
    "                    \"verbosity\": -1,\n",
    "                    \"random_state\": SEED,\n",
    "                    \"n_jobs\": -1,\n",
    "                    \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "                    \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "                    \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "                    \"feature_fraction\": trial.suggest_float(\n",
    "                        \"feature_fraction\", 0.4, 1.0\n",
    "                    ),\n",
    "                    \"bagging_fraction\": trial.suggest_float(\n",
    "                        \"bagging_fraction\", 0.4, 1.0\n",
    "                    ),\n",
    "                    \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "                    \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "                }\n",
    "                model = LGBMClassifier(**params)\n",
    "\n",
    "            if self.run_group_name == \"catboost\":\n",
    "                params = {\n",
    "                    \"random_seed\": SEED,\n",
    "                    \"verbose\": 0,\n",
    "                    \"allow_writing_files\": False,\n",
    "                    \"colsample_bylevel\": trial.suggest_float(\n",
    "                        \"colsample_bylevel\", 0.01, 0.1\n",
    "                    ),\n",
    "                    \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "                    \"boosting_type\": trial.suggest_categorical(\n",
    "                        \"boosting_type\", [\"Ordered\", \"Plain\"]\n",
    "                    ),\n",
    "                    \"bootstrap_type\": trial.suggest_categorical(\n",
    "                        \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "                    ),\n",
    "                }\n",
    "                model = CatBoostClassifier(**params)\n",
    "\n",
    "            model.fit(X=self.X_train, y=self.y_train)\n",
    "\n",
    "            # calculating the training f1 score\n",
    "            train_prediction = model.predict(self.X_train)\n",
    "            train_f1 = f1_score(\n",
    "                y_true=self.y_train, y_pred=train_prediction, average=\"weighted\"\n",
    "            )\n",
    "\n",
    "            # calculating the validation f1 score\n",
    "            valid_prediction = model.predict(self.X_valid)\n",
    "            valid_f1 = f1_score(\n",
    "                y_true=self.y_valid, y_pred=valid_prediction, average=\"weighted\"\n",
    "            )\n",
    "\n",
    "            # logging the training and validation scores\n",
    "            mlflow.log_metric(\"train_f1\", train_f1)\n",
    "            mlflow.log_metric(\"valid_f1\", valid_f1)\n",
    "\n",
    "            # inferring the signature of the trained model\n",
    "            signature = infer_signature(\n",
    "                model_input=self.X_train, model_output=train_prediction\n",
    "            )\n",
    "\n",
    "            # saving the trained model\n",
    "            if self.run_group_name in [\"decision_tree\", \"random_forest\"]:\n",
    "                # sklearn flavor\n",
    "                mlflow.sklearn.log_model(\n",
    "                    model, self.run_group_name, signature=signature\n",
    "                )\n",
    "                # logging the model\"s default parameters\n",
    "                mlflow.log_params(model.get_params(deep=True))\n",
    "            elif self.run_group_name == \"xgboost\":\n",
    "                mlflow.xgboost.log_model(\n",
    "                    model, self.run_group_name, signature=signature\n",
    "                )\n",
    "                # logging the model's default parameters\n",
    "                mlflow.log_params(model.get_params())\n",
    "            elif self.run_group_name == \"lightgbm\":\n",
    "                mlflow.lightgbm.log_model(\n",
    "                    model, self.run_group_name, signature=signature\n",
    "                )\n",
    "                # logging the model's default parameters\n",
    "                mlflow.log_params(model.get_params())\n",
    "            elif self.run_group_name == \"catboost\":\n",
    "                mlflow.catboost.log_model(\n",
    "                    model, self.run_group_name, signature=signature\n",
    "                )\n",
    "                # logging the model's default parameters\n",
    "                mlflow.log_params(model.get_all_params())\n",
    "\n",
    "        return valid_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new mlflow's experiment\n",
    "hpt_experiment_id = mlflow.create_experiment(\n",
    "    name=HYPERPARAMETER_TUNING_EXPERIMENT_NAME + \"_\" + datetime_string,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run optuna trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_model_params(model_name):\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    latest_version_info = client.get_latest_versions(model_name)[0]\n",
    "    run_id = latest_version_info.run_id\n",
    "    run = client.get_run(run_id)\n",
    "\n",
    "    # Get the parameters logged for the run\n",
    "    parameters = run.data.params\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def run_mlflow_experiment(run_group_name, experiment_id, direction=\"maximize\"):\n",
    "    dt_run_params = get_latest_model_params(\n",
    "        FEATURE_SELECTION_EXPERIMENT_NAME + \"_\" + run_group_name\n",
    "    )\n",
    "    dt_features_indexes = ast.literal_eval(dt_run_params[\"features\"])\n",
    "\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name=run_group_name):\n",
    "        objective = Objective(\n",
    "            run_group_name=run_group_name,\n",
    "            experiment_id=experiment_id,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_valid=X_valid,\n",
    "            y_valid=y_valid,\n",
    "            indexes=dt_features_indexes,\n",
    "        )\n",
    "\n",
    "        study = optuna.create_study(direction=direction)\n",
    "        study.optimize(objective, n_trials=10)\n",
    "\n",
    "        return study.trials_dataframe()\n",
    "\n",
    "\n",
    "all_trials_dfs = []\n",
    "run_group_names = [\"decision_tree\", \"random_forest\", \"xgboost\", \"lightgbm\", \"catboost\"]\n",
    "for run_group_name in run_group_names:\n",
    "    trials_df = run_mlflow_experiment(\n",
    "        run_group_name, experiment_id, direction=\"maximize\"\n",
    "    )\n",
    "    all_trials_dfs.append(trials_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze all trial results, and register best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials = pd.concat(all_trials_dfs, axis=0)\n",
    "best_trial_id = df_trials[\"value\"].idxmax()\n",
    "best_trial = df_trials.iloc[best_trial_id]\n",
    "\n",
    "results = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{best_trial['user_attrs_run_id']}/{best_trial['user_attrs_run_name']}\",\n",
    "    name=f\"experimentation-best-model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if credentials_config[\"EC2\"] != \"YOUR_EC2_INSTANCE_URL\":\n",
    "    # removing downloaded dataset from local\n",
    "    os.remove(PROCESSED_RAW_FILE_PATH)\n",
    "\n",
    "    # removing the local artifacts and features\n",
    "    shutil.rmtree(ARTIFACTS_OUTPUT_PATH)\n",
    "    shutil.rmtree(FEATURES_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b2e12d173146cb936f2afc8a1d3689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "logged_model = \"runs:/464064cef6794b918e3b432f8587e674/decision_tree\"\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39f457c11134e1fb8ced5e6e5f069f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MlflowException",
     "evalue": "The following failures occurred while downloading one or more artifacts from http://mlflow:5000/api/2.0/mlflow-artifacts/artifacts/666474066399758848/464064cef6794b918e3b432f8587e674/artifacts/decision_tree_trial_2:\n##### File  #####\nAPI request to http://mlflow:5000/api/2.0/mlflow-artifacts/artifacts/666474066399758848/464064cef6794b918e3b432f8587e674/artifacts/decision_tree_trial_2/ failed with exception HTTPConnectionPool(host='mlflow', port=5000): Max retries exceeded with url: /api/2.0/mlflow-artifacts/artifacts/666474066399758848/464064cef6794b918e3b432f8587e674/artifacts/decision_tree_trial_2/ (Caused by ResponseError('too many 500 error responses'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tracking_uri(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://mlflow:5000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels:/experimentation-best-model/4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/_dev/lib/python3.9/site-packages/mlflow/tracing/provider.py:253\u001b[0m, in \u001b[0;36mtrace_disabled.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m disable()\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     is_func_called, result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     enable()\n",
      "File \u001b[0;32m/opt/venv/_dev/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py:1017\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_uri, suppress_warnings, dst_path, model_config)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         entity_list\u001b[38;5;241m.\u001b[39mappend(Entity(job\u001b[38;5;241m=\u001b[39mjob_entity))\n\u001b[1;32m   1015\u001b[0m     lineage_header_info \u001b[38;5;241m=\u001b[39m LineageHeaderInfo(entities\u001b[38;5;241m=\u001b[39mentity_list) \u001b[38;5;28;01mif\u001b[39;00m entity_list \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m local_path \u001b[38;5;241m=\u001b[39m \u001b[43m_download_artifact_from_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdst_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlineage_header_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineage_header_info\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m suppress_warnings:\n\u001b[1;32m   1022\u001b[0m     model_requirements \u001b[38;5;241m=\u001b[39m _get_pip_requirements_from_model_path(local_path)\n",
      "File \u001b[0;32m/opt/venv/_dev/lib/python3.9/site-packages/mlflow/tracking/artifact_utils.py:111\u001b[0m, in \u001b[0;36m_download_artifact_from_uri\u001b[0;34m(artifact_uri, output_path, lineage_header_info)\u001b[0m\n\u001b[1;32m    108\u001b[0m repo \u001b[38;5;241m=\u001b[39m get_artifact_repository(artifact_uri\u001b[38;5;241m=\u001b[39mroot_uri)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(repo, ModelsArtifactRepository):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlineage_header_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineage_header_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repo\u001b[38;5;241m.\u001b[39mdownload_artifacts(artifact_path\u001b[38;5;241m=\u001b[39martifact_path, dst_path\u001b[38;5;241m=\u001b[39moutput_path)\n",
      "File \u001b[0;32m/opt/venv/_dev/lib/python3.9/site-packages/mlflow/store/artifact/models_artifact_repo.py:190\u001b[0m, in \u001b[0;36mModelsArtifactRepository.download_artifacts\u001b[0;34m(self, artifact_path, dst_path, lineage_header_info)\u001b[0m\n\u001b[1;32m    186\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo\u001b[38;5;241m.\u001b[39mdownload_artifacts(\n\u001b[1;32m    187\u001b[0m         artifact_path, dst_path, lineage_header_info\u001b[38;5;241m=\u001b[39mlineage_header_info\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# NB: only add the registered model metadata iff the artifact path is at the root model\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# directory. For individual files or subdirectories within the model directory, do not\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# create the metadata file.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(model_path) \u001b[38;5;129;01mand\u001b[39;00m MLMODEL_FILE_NAME \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(model_path):\n",
      "File \u001b[0;32m/opt/venv/_dev/lib/python3.9/site-packages/mlflow/store/artifact/artifact_repo.py:284\u001b[0m, in \u001b[0;36mArtifactRepository.download_artifacts\u001b[0;34m(self, artifact_path, dst_path)\u001b[0m\n\u001b[1;32m    278\u001b[0m         template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m##### File \u001b[39m\u001b[38;5;132;01m{path}\u001b[39;00m\u001b[38;5;124m #####\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{error}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    281\u001b[0m         template\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mpath, error\u001b[38;5;241m=\u001b[39merror, traceback\u001b[38;5;241m=\u001b[39mtracebacks[path])\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m path, error \u001b[38;5;129;01min\u001b[39;00m failed_downloads\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    283\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    285\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    286\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following failures occurred while downloading one or more\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m artifacts from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39martifact_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_truncate_error(failures)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m         )\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst_path, artifact_path)\n",
      "\u001b[0;31mMlflowException\u001b[0m: The following failures occurred while downloading one or more artifacts from http://mlflow:5000/api/2.0/mlflow-artifacts/artifacts/666474066399758848/464064cef6794b918e3b432f8587e674/artifacts/decision_tree_trial_2:\n##### File  #####\nAPI request to http://mlflow:5000/api/2.0/mlflow-artifacts/artifacts/666474066399758848/464064cef6794b918e3b432f8587e674/artifacts/decision_tree_trial_2/ failed with exception HTTPConnectionPool(host='mlflow', port=5000): Max retries exceeded with url: /api/2.0/mlflow-artifacts/artifacts/666474066399758848/464064cef6794b918e3b432f8587e674/artifacts/decision_tree_trial_2/ (Caused by ResponseError('too many 500 error responses'))"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(f\"http://mlflow:5000\")\n",
    "model_uri = \"models:/experimentation-best-model/4\"\n",
    "model = mlflow.pyfunc.load_model(model_uri=model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
